{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6j+R+ivhibqBFbn8UDr6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kchuri01/Bayesian-Neural-Network-CHD-Risk/blob/main/bnn_framingham.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G6DUGqW0PrsB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "DATA_PATH = \"/content/framingham.csv.xls\""
      ],
      "metadata": {
        "id": "AAPsB6PHQFhV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATA_PATH, na_values=[\"NA\"])\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FmlezVNnQRS-",
        "outputId": "06e2d04d-a94f-4b00-f1ea-ce32d397196b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
            "0     1   39        4.0              0         0.0     0.0                0   \n",
            "1     0   46        2.0              0         0.0     0.0                0   \n",
            "2     1   48        1.0              1        20.0     0.0                0   \n",
            "3     0   61        3.0              1        30.0     0.0                0   \n",
            "4     0   46        3.0              1        23.0     0.0                0   \n",
            "\n",
            "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
            "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
            "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
            "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
            "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
            "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
            "\n",
            "   TenYearCHD  \n",
            "0           0  \n",
            "1           0  \n",
            "2           0  \n",
            "3           1  \n",
            "4           0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4238 entries, 0 to 4237\n",
            "Data columns (total 16 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   male             4238 non-null   int64  \n",
            " 1   age              4238 non-null   int64  \n",
            " 2   education        4133 non-null   float64\n",
            " 3   currentSmoker    4238 non-null   int64  \n",
            " 4   cigsPerDay       4209 non-null   float64\n",
            " 5   BPMeds           4185 non-null   float64\n",
            " 6   prevalentStroke  4238 non-null   int64  \n",
            " 7   prevalentHyp     4238 non-null   int64  \n",
            " 8   diabetes         4238 non-null   int64  \n",
            " 9   totChol          4188 non-null   float64\n",
            " 10  sysBP            4238 non-null   float64\n",
            " 11  diaBP            4238 non-null   float64\n",
            " 12  BMI              4219 non-null   float64\n",
            " 13  heartRate        4237 non-null   float64\n",
            " 14  glucose          3850 non-null   float64\n",
            " 15  TenYearCHD       4238 non-null   int64  \n",
            "dtypes: float64(9), int64(7)\n",
            "memory usage: 529.9 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"TenYearCHD\"\n",
        "assert TARGET in df.columns, f\"Target '{TARGET}' not found. Columns: {df.columns.tolist()}\""
      ],
      "metadata": {
        "id": "ZrQ9ImlZQe5i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features/target\n",
        "y = df[TARGET].astype(float).values\n",
        "X = df.drop(columns=[TARGET])"
      ],
      "metadata": {
        "id": "jKWwh15_QiCk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in X.columns:\n",
        "    if X[col].dtype == \"object\":\n",
        "        X[col] = X[col].astype(str).str.lower().map({\"male\": 1.0, \"m\": 1.0, \"female\": 0.0, \"f\": 0.0})\n",
        "        # if mapping fails, fall back to factorize\n",
        "        if X[col].isna().any():\n",
        "            X[col] = pd.factorize(df[col].astype(str))[0].astype(float)\n",
        "\n",
        "X = X.astype(float).values"
      ],
      "metadata": {
        "id": "uF18lBtHQkXQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test split + preprocessing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "KXuRyg6gQm5e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to torch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "jipPia1BQpwk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class imbalance handling\n",
        "pos_rate = float(y_train.mean())\n",
        "# pos_weight > 1 increases penalty on positive misclassification\n",
        "pos_weight = torch.tensor([(1 - pos_rate) / max(pos_rate, 1e-6)], device=device)"
      ],
      "metadata": {
        "id": "m0q1MRrJQsde"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Layers (Variational)\n",
        "class BayesLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, prior_std=1.0):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Variational parameters\n",
        "        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))\n",
        "        self.weight_rho = nn.Parameter(torch.full((out_features, in_features), -5.0))\n",
        "        self.bias_mu = nn.Parameter(torch.zeros(out_features))\n",
        "        self.bias_rho = nn.Parameter(torch.full((out_features,), -5.0))\n",
        "\n",
        "        # Prior parameters\n",
        "        self.prior_std = prior_std\n",
        "\n",
        "    def _sigma(self, rho):\n",
        "        return torch.log1p(torch.exp(rho))\n",
        "\n",
        "    def kl_divergence(self):\n",
        "\n",
        "        w_sigma = self._sigma(self.weight_rho)\n",
        "        b_sigma = self._sigma(self.bias_rho)\n",
        "\n",
        "        prior_var = self.prior_std ** 2\n",
        "        w_var = w_sigma ** 2\n",
        "        b_var = b_sigma ** 2\n",
        "\n",
        "        # KL for each parameter then sum\n",
        "        kl_w = 0.5 * torch.sum((w_var + self.weight_mu**2) / prior_var - 1.0 - torch.log(w_var / prior_var))\n",
        "        kl_b = 0.5 * torch.sum((b_var + self.bias_mu**2) / prior_var - 1.0 - torch.log(b_var / prior_var))\n",
        "        return kl_w + kl_b\n",
        "\n",
        "    def forward(self, x, sample=True):\n",
        "        if sample:\n",
        "            w_sigma = self._sigma(self.weight_rho)\n",
        "            b_sigma = self._sigma(self.bias_rho)\n",
        "\n",
        "            # Reparameterisation\n",
        "            eps_w = torch.randn_like(self.weight_mu)\n",
        "            eps_b = torch.randn_like(self.bias_mu)\n",
        "            weight = self.weight_mu + w_sigma * eps_w\n",
        "            bias = self.bias_mu + b_sigma * eps_b\n",
        "        else:\n",
        "            # Use posterior mean for deterministic pass\n",
        "            weight = self.weight_mu\n",
        "            bias = self.bias_mu\n",
        "\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "class BayesianMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim=16, prior_std=1.0):\n",
        "        super().__init__()\n",
        "        self.b1 = BayesLinear(in_dim, hidden_dim, prior_std=prior_std)\n",
        "        self.b2 = BayesLinear(hidden_dim, 1, prior_std=prior_std)\n",
        "\n",
        "    def kl(self):\n",
        "        return self.b1.kl_divergence() + self.b2.kl_divergence()\n",
        "\n",
        "    def forward(self, x, sample=True):\n",
        "        x = self.b1(x, sample=sample)\n",
        "        x = torch.relu(x)\n",
        "        logits = self.b2(x, sample=sample).squeeze(-1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "EWG5E0cRQxtZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with ELBO (NLL + KL)\n",
        "model = BayesianMLP(in_dim=X_train.shape[1], hidden_dim=16, prior_std=1.0).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Bernoulli likelihood to BCEWithLogitsLoss (numerically stable)\n",
        "bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "def train_epoch(model, loader, beta=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_nll = 0.0\n",
        "    total_kl = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(xb, sample=True)\n",
        "        nll = bce(logits, yb)\n",
        "\n",
        "        # Scale KL by dataset size\n",
        "        kl = model.kl() / X_train_t.shape[0]\n",
        "\n",
        "        loss = nll + beta * kl\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        total_loss += float(loss.item()) * bs\n",
        "        total_nll += float(nll.item()) * bs\n",
        "        total_kl += float(kl.item()) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_nll / n, total_kl / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def mc_predict_proba(model, X, mc_samples=200):\n",
        "\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    for _ in range(mc_samples):\n",
        "        logits = model(X, sample=True)\n",
        "        probs.append(torch.sigmoid(logits).unsqueeze(0))  # [1, N]\n",
        "    probs = torch.cat(probs, dim=0)                       # [S, N]\n",
        "    return probs       # return all samples to compute uncertainty\n",
        "\n",
        "# Train\n",
        "EPOCHS = 80\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss, nll, kl = train_epoch(model, train_loader, beta=1.0)\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | NLL {nll:.4f} | KL {kl:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Sk8mGKNuQyzN",
        "outputId": "e12d5114-9b74-42f4-b275-870ba6dd350a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Loss 1.5365 | NLL 1.1748 | KL 0.361644\n",
            "Epoch 010 | Loss 1.3690 | NLL 1.0260 | KL 0.343074\n",
            "Epoch 020 | Loss 1.3340 | NLL 1.0111 | KL 0.322900\n",
            "Epoch 030 | Loss 1.3122 | NLL 1.0087 | KL 0.303557\n",
            "Epoch 040 | Loss 1.2910 | NLL 1.0059 | KL 0.285126\n",
            "Epoch 050 | Loss 1.2723 | NLL 1.0047 | KL 0.267609\n",
            "Epoch 060 | Loss 1.2524 | NLL 1.0010 | KL 0.251422\n",
            "Epoch 070 | Loss 1.2370 | NLL 1.0004 | KL 0.236669\n",
            "Epoch 080 | Loss 1.2222 | NLL 0.9987 | KL 0.223514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate (AUC/Accuracy) + Uncertainty\n",
        "probs_samps = mc_predict_proba(model, X_test_t, mc_samples=300)  # [S, N]\n",
        "probs_mean = probs_samps.mean(dim=0).cpu().numpy()               # [N]\n",
        "probs_std  = probs_samps.std(dim=0).cpu().numpy()                # [N]\n",
        "\n",
        "y_true = y_test_t.cpu().numpy()\n",
        "\n",
        "auc = roc_auc_score(y_true, probs_mean)\n",
        "pred = (probs_mean >= 0.5).astype(int)\n",
        "acc = accuracy_score(y_true, pred)\n",
        "\n",
        "# 95% credible interval (approx) from MC samples\n",
        "lower = np.quantile(probs_samps.cpu().numpy(), 0.025, axis=0)\n",
        "upper = np.quantile(probs_samps.cpu().numpy(), 0.975, axis=0)\n",
        "\n",
        "print(\"\\n--- Test Performance ---\")\n",
        "print(f\"ROC-AUC:  {auc:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "# A few predictions with uncertainty\n",
        "out = pd.DataFrame({\"y_true\": y_true.astype(int), \"p_mean\": probs_mean,\n",
        "                    \"p_std\": probs_std, \"p_2.5%\": lower, \"p_97.5%\": upper})\n",
        "\n",
        "print(\"\\n--- Sample Predictions (with uncertainty) ---\")\n",
        "print(out.sort_values(\"p_std\", ascending=False).head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JuyzXjjdQ6km",
        "outputId": "61917f13-12ca-4b3e-ae8a-e4b8f2f02725"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Performance ---\n",
            "ROC-AUC:  0.6948\n",
            "Accuracy: 0.6474\n",
            "\n",
            "--- Sample Predictions (with uncertainty) ---\n",
            " y_true   p_mean    p_std   p_2.5%  p_97.5%\n",
            "      0 0.702786 0.068225 0.579354 0.822898\n",
            "      0 0.799691 0.053340 0.682819 0.891626\n",
            "      1 0.638588 0.049528 0.541555 0.750512\n",
            "      1 0.626004 0.046145 0.531368 0.710149\n",
            "      0 0.425514 0.045079 0.339676 0.512058\n",
            "      0 0.281426 0.041884 0.212521 0.377711\n",
            "      0 0.757855 0.040245 0.674695 0.827601\n",
            "      1 0.844172 0.040206 0.755036 0.905858\n",
            "      0 0.568484 0.040086 0.491572 0.649084\n",
            "      0 0.654470 0.040011 0.584784 0.732826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncertain cases\n",
        "# flag cases where CI crosses 0.5 (model not confident about class)\n",
        "out[\"ci_crosses_0.5\"] = (out[\"p_2.5%\"] < 0.5) & (out[\"p_97.5%\"] > 0.5)\n",
        "print(\"\\nUncertain cases (CI crosses 0.5):\", int(out[\"ci_crosses_0.5\"].sum()), \"out of\", len(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qOAZD1IjRCqU",
        "outputId": "b2e33464-1f80-4212-9a1b-1cbc1725f8a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uncertain cases (CI crosses 0.5): 97 out of 848\n"
          ]
        }
      ]
    }
  ]
}